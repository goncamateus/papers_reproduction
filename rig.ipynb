{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from models.bvae import BetaVAE\n",
    "from models.ddpg import OUNoise\n",
    "from models.sac import GaussianPolicy, QNetwork\n",
    "from utils.memory import ReplayBuffer\n",
    "from utils.sync import soft_sync\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v1')"
   ]
  },
  {
   "source": [
    "## Vae training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_training = list()\n",
    "pre_training.append(env.reset()['observation'])\n",
    "for _ in range(100):\n",
    "    status = env.reset()\n",
    "    state = status['observation']\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = next_state['observation']\n",
    "        pre_training.append(next_state)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=10, out_features=400, bias=True)\n",
       "  (fc21): Linear(in_features=400, out_features=5, bias=True)\n",
       "  (fc22): Linear(in_features=400, out_features=5, bias=True)\n",
       "  (fc3): Linear(in_features=5, out_features=400, bias=True)\n",
       "  (fc4): Linear(in_features=400, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "vae_model = BetaVAE(env.observation_space['observation'].shape[0],\n",
    "                    env.observation_space['observation'].shape[0]//2)\n",
    "vae_model = vae_model.to(torch.device('cuda'))\n",
    "vae_optim = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "vae_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Loss: 0.23852225937522376\n",
      "Train Loss: -0.07086417240162309\n",
      "Train Loss: -0.31805492268923\n",
      "Train Loss: -0.6278284910397652\n",
      "Train Loss: -0.738910334232526\n",
      "Train Loss: -0.777806621331435\n",
      "Train Loss: -0.792729261593941\n",
      "Train Loss: -0.8013213536678216\n",
      "Train Loss: -0.8050079758350666\n",
      "Train Loss: -0.8104759515860142\n",
      "Train Loss: -0.8127880906447386\n",
      "Train Loss: -0.8138412191317632\n",
      "Train Loss: -0.8146612659478799\n",
      "Train Loss: -0.8151785777165339\n",
      "Train Loss: -0.8152213142468379\n",
      "Train Loss: -0.8160267243018517\n",
      "Train Loss: -0.8166583883456695\n",
      "Train Loss: -0.8171907724478306\n",
      "Train Loss: -0.8164930664576017\n",
      "Train Loss: -0.8174881369639666\n",
      "Train Loss: -0.8172295506183918\n",
      "Train Loss: -0.8176563534981165\n",
      "Train Loss: -0.8178965556315887\n",
      "Train Loss: -0.81769859790802\n",
      "Train Loss: -0.8179870721621391\n",
      "Train Loss: -0.8180087040632199\n",
      "Train Loss: -0.8175732906048114\n",
      "Train Loss: -0.8175020447144141\n",
      "Train Loss: -0.8178203808955657\n",
      "Train Loss: -0.818039440191709\n",
      "Train Loss: -0.8177252793923403\n",
      "Train Loss: -0.8183047022574987\n",
      "Train Loss: -0.8183694497132913\n",
      "Train Loss: -0.8182077209154764\n",
      "Train Loss: -0.8180894225071638\n",
      "Train Loss: -0.8179389528739147\n",
      "Train Loss: -0.8181746968856225\n",
      "Train Loss: -0.8184177844952314\n",
      "Train Loss: -0.8183013460575006\n",
      "Train Loss: -0.8183969045296694\n",
      "Train Loss: -0.8183373564328903\n",
      "Train Loss: -0.8183900515238444\n",
      "Train Loss: -0.8184389472007751\n",
      "Train Loss: -0.8186436203809885\n",
      "Train Loss: -0.8183462589215009\n",
      "Train Loss: -0.8183979926965176\n",
      "Train Loss: -0.8183368734824352\n",
      "Train Loss: -0.818499260987991\n",
      "Train Loss: -0.8183364959863516\n",
      "Train Loss: -0.8184678325286279\n"
     ]
    }
   ],
   "source": [
    "def train_vae(train_data):\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_data):\n",
    "        data = torch.FloatTensor(data).to(torch.device('cuda'))\n",
    "        vae_optim.zero_grad()\n",
    "        results = vae_model(data)\n",
    "        loss = vae_model.loss_function(*results, M_N=1/len(train_data))\n",
    "        loss['loss'].backward()\n",
    "        train_loss += loss['loss'].item()\n",
    "        vae_optim.step()\n",
    "    print('Train Loss:', train_loss/len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    batches = np.array_split(np.array(pre_training), len(pre_training)//128)\n",
    "    train_vae(batches)"
   ]
  },
  {
   "source": [
    "## Training policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "target_entropy = - \\\n",
    "    torch.prod(torch.Tensor(env.action_space.shape).to(device)).item()\n",
    "log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "alpha_optim = optim.Adam([log_alpha], lr=lr)\n",
    "\n",
    "policy = GaussianPolicy((env.observation_space['observation'].shape[0]//2)*2,\n",
    "                        env.action_space.shape[0]).to(device)\n",
    "\n",
    "crt = QNetwork((env.observation_space['observation'].shape[0]//2)\n",
    "               * 2, env.action_space.shape[0]).to(device)\n",
    "tgt_crt = QNetwork((env.observation_space['observation'].shape[0]//2)*2,\n",
    "                   env.action_space.shape[0]).to(device)\n",
    "\n",
    "tgt_crt.load_state_dict(crt.state_dict())\n",
    "\n",
    "policy_optim = optim.Adam(policy.parameters(), lr=lr)\n",
    "crt_optim = optim.Adam(crt.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = OUNoise(env.action_space)\n",
    "memory = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    x = x.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "    res = np.linalg.norm(x-y, axis=1)\n",
    "    return torch.tensor(res).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(act_net, crt_net, tgt_crt_net,\n",
    "                 optimizer_act, optimizer_crt,\n",
    "                 memory, vae_model, batch_size=128,\n",
    "                 automatic_entropy_tuning=True):\n",
    "    global alpha, log_alpha, alpha_optim\n",
    "    gamma = 0.99\n",
    "    state_batch, action_batch, reward_batch,\\\n",
    "        next_state_batch, mask_batch, goal_batch = memory.sample(batch_size)\n",
    "\n",
    "    state_batch = torch.FloatTensor(state_batch).to(device)\n",
    "    goal_batch = torch.FloatTensor(goal_batch).to(device)\n",
    "    next_state_batch = torch.FloatTensor(\n",
    "        next_state_batch).to(device)\n",
    "    action_batch = torch.FloatTensor(action_batch).to(device)\n",
    "\n",
    "    reward_batch = torch.FloatTensor(\n",
    "        reward_batch).to(device).unsqueeze(1)\n",
    "    reward_batch = - dist(next_state_batch, goal_batch)\n",
    "\n",
    "    mask_batch = torch.BoolTensor(\n",
    "        mask_batch).to(device).unsqueeze(1)\n",
    "\n",
    "    # state_batch, logvar = vae_model.encode(state_batch)\n",
    "    # state_batch, logvar = state_batch.detach(), logvar.detach()\n",
    "    # next_state_batch = vae_model.encode(next_state_batch)[0].detach()\n",
    "\n",
    "    # if np.random.rand() > 0.5:\n",
    "    #     goal_batch = vae_model.reparameterize(state_batch, logvar)\n",
    "\n",
    "    state_batch = torch.cat((state_batch, goal_batch), 1)\n",
    "    next_state_batch = torch.cat((next_state_batch, goal_batch), 1)\n",
    "    for i, (state, action, next_state, done) in enumerate(episode):\n",
    "        for t in np.random.choice(len(episode), 20):\n",
    "            s_hi = episode[t][-2]\n",
    "            s_hi, _ = vae_model.encode(\n",
    "                torch.FloatTensor(next_state).to(device))\n",
    "            s_hi = s_hi.detach().cpu().numpy()\n",
    "            mu, _ = vae_model.encode(torch.FloatTensor(state).to(device))\n",
    "            mu = mu.detach().cpu().numpy()\n",
    "            memory.push(mu, action, 0, s_hi, done, s_hi)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_action, next_state_log_pi, _ = act_net.sample(\n",
    "            next_state_batch)\n",
    "        qf1_next_target, qf2_next_target = tgt_crt_net(\n",
    "            next_state_batch, next_state_action)\n",
    "        min_qf_next_target = torch.min(\n",
    "            qf1_next_target,\n",
    "            qf2_next_target) - alpha * next_state_log_pi\n",
    "        min_qf_next_target[mask_batch] = 0.0\n",
    "        next_q_value = reward_batch + gamma * (min_qf_next_target)\n",
    "    # Two Q-functions to mitigate\n",
    "    # positive bias in the policy improvement step\n",
    "    qf1, qf2 = crt_net(state_batch, action_batch)\n",
    "    # JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\n",
    "    qf1_loss = F.mse_loss(qf1, next_q_value.detach())\n",
    "    # JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\n",
    "    qf2_loss = F.mse_loss(qf2, next_q_value.detach())\n",
    "\n",
    "    pi, log_pi, _ = act_net.sample(state_batch)\n",
    "\n",
    "    qf1_pi, qf2_pi = crt_net(state_batch, pi)\n",
    "    min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "    # JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "    policy_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "    optimizer_crt.zero_grad()\n",
    "    qf1_loss.backward()\n",
    "    optimizer_crt.step()\n",
    "\n",
    "    optimizer_crt.zero_grad()\n",
    "    qf2_loss.backward()\n",
    "    optimizer_crt.step()\n",
    "\n",
    "    optimizer_act.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer_act.step()\n",
    "\n",
    "    if automatic_entropy_tuning:\n",
    "        alpha_loss = -(log_alpha * (log_pi +\n",
    "                                    target_entropy\n",
    "                                    ).detach()).mean()\n",
    "\n",
    "        alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        alpha_optim.step()\n",
    "\n",
    "        alpha = log_alpha.exp()\n",
    "    else:\n",
    "        alpha_loss = torch.tensor(0.).to(device)\n",
    "\n",
    "    return policy_loss.item(), qf1_loss.item(), \\\n",
    "        qf2_loss.item(), alpha_loss.item()\n",
    "\n",
    "\n",
    "def select_action(policy, state, evaluate=False):\n",
    "    state = state.unsqueeze(0)\n",
    "    if evaluate is False:\n",
    "        action, _, _ = policy.sample(state)\n",
    "    else:\n",
    "        _, _, action = policy.sample(state)\n",
    "    return action.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae = collections.deque(maxlen=500000)\n",
    "for data in pre_training:\n",
    "    data_vae.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 0 -> Reward: -50.0\n",
      "Episode 1 -> Reward: -50.0\n",
      "Episode 2 -> Reward: -50.0\n",
      "Episode 3 -> Reward: -50.0\n",
      "Episode 4 -> Reward: -49.0\n",
      "Episode 5 -> Reward: -50.0\n",
      "Episode 6 -> Reward: -50.0\n",
      "Episode 7 -> Reward: -50.0\n",
      "Episode 8 -> Reward: -50.0\n",
      "Episode 9 -> Reward: -50.0\n",
      "Episode 10 -> Reward: -50.0\n",
      "Train Loss: -0.8048813879489899\n",
      "Train Loss: -0.8050292730331421\n",
      "Train Loss: -0.8052889287471772\n",
      "Train Loss: -0.8050947844982147\n",
      "Train Loss: -0.8056789755821228\n",
      "Train Loss: -0.8053813457489014\n",
      "Train Loss: -0.8051162898540497\n",
      "Train Loss: -0.8053756833076477\n",
      "Train Loss: -0.8051999628543853\n",
      "Train Loss: -0.8054827213287353\n",
      "Train Loss: -0.8053594946861267\n",
      "Train Loss: -0.8054150402545929\n",
      "Train Loss: -0.805502200126648\n",
      "Train Loss: -0.8052301943302155\n",
      "Train Loss: -0.8051408231258392\n",
      "Train Loss: -0.805518639087677\n",
      "Train Loss: -0.8054404020309448\n",
      "Train Loss: -0.8055746197700501\n",
      "Train Loss: -0.8055885672569275\n",
      "Train Loss: -0.805409038066864\n",
      "Train Loss: -0.8054190218448639\n",
      "Train Loss: -0.8050252199172974\n",
      "Train Loss: -0.8055930614471436\n",
      "Train Loss: -0.8054260432720184\n",
      "Train Loss: -0.8054936408996582\n",
      "Train Loss: -0.8054834544658661\n",
      "Train Loss: -0.8055588364601135\n",
      "Train Loss: -0.8053669393062591\n",
      "Train Loss: -0.805558180809021\n",
      "Train Loss: -0.8056679606437683\n",
      "Train Loss: -0.805636602640152\n",
      "Train Loss: -0.8055120766162872\n",
      "Train Loss: -0.8056339800357819\n",
      "Train Loss: -0.8052776396274567\n",
      "Train Loss: -0.8055322885513305\n",
      "Train Loss: -0.8055773198604583\n",
      "Train Loss: -0.8057083070278168\n",
      "Train Loss: -0.8055586397647858\n",
      "Train Loss: -0.8053280174732208\n",
      "Train Loss: -0.8056328594684601\n",
      "Train Loss: -0.8055025279521942\n",
      "Train Loss: -0.8058166325092315\n",
      "Train Loss: -0.8057348191738128\n",
      "Train Loss: -0.805386084318161\n",
      "Train Loss: -0.8052308142185212\n",
      "Train Loss: -0.8052287995815277\n",
      "Train Loss: -0.8054149627685547\n",
      "Train Loss: -0.8054883360862732\n",
      "Train Loss: -0.8056721806526184\n",
      "Train Loss: -0.8057235658168793\n",
      "Episode 11 -> Reward: -50.0\n",
      "Episode 12 -> Reward: -50.0\n",
      "Episode 13 -> Reward: -50.0\n",
      "Episode 14 -> Reward: -49.0\n",
      "Episode 15 -> Reward: -48.0\n",
      "Episode 16 -> Reward: -50.0\n",
      "Episode 17 -> Reward: -50.0\n",
      "Episode 18 -> Reward: -50.0\n",
      "Episode 19 -> Reward: -50.0\n",
      "Episode 20 -> Reward: -50.0\n",
      "Train Loss: -0.7794031023979187\n",
      "Train Loss: -0.7796211779117584\n",
      "Train Loss: -0.7796043515205383\n",
      "Train Loss: -0.7795813918113709\n",
      "Train Loss: -0.7795173585414886\n",
      "Train Loss: -0.7791036903858185\n",
      "Train Loss: -0.7796208143234253\n",
      "Train Loss: -0.7796710789203644\n",
      "Train Loss: -0.7798186063766479\n",
      "Train Loss: -0.7798705518245697\n",
      "Train Loss: -0.7794503927230835\n",
      "Train Loss: -0.7798791468143463\n",
      "Train Loss: -0.7791137516498565\n",
      "Train Loss: -0.7795354068279267\n",
      "Train Loss: -0.7797931015491486\n",
      "Train Loss: -0.7795936107635498\n",
      "Train Loss: -0.7794314801692963\n",
      "Train Loss: -0.7793724536895752\n",
      "Train Loss: -0.7792303025722503\n",
      "Train Loss: -0.7796741187572479\n",
      "Train Loss: -0.7798020780086518\n",
      "Train Loss: -0.7794673025608063\n",
      "Train Loss: -0.7799032032489777\n",
      "Train Loss: -0.7793249666690827\n",
      "Train Loss: -0.7799384415149688\n",
      "Train Loss: -0.7797999918460846\n",
      "Train Loss: -0.7796335220336914\n",
      "Train Loss: -0.7799394428730011\n",
      "Train Loss: -0.7796230375766754\n",
      "Train Loss: -0.7796047329902649\n",
      "Train Loss: -0.7798068940639495\n",
      "Train Loss: -0.7797064423561096\n",
      "Train Loss: -0.780241847038269\n",
      "Train Loss: -0.7801444172859192\n",
      "Train Loss: -0.77984499335289\n",
      "Train Loss: -0.780184268951416\n",
      "Train Loss: -0.7797713875770569\n",
      "Train Loss: -0.7795843243598938\n",
      "Train Loss: -0.7800781011581421\n",
      "Train Loss: -0.779797226190567\n",
      "Train Loss: -0.7801015317440033\n",
      "Train Loss: -0.7797348439693451\n",
      "Train Loss: -0.7800344824790955\n",
      "Train Loss: -0.7799903988838196\n",
      "Train Loss: -0.7796738505363464\n",
      "Train Loss: -0.7798927068710327\n",
      "Train Loss: -0.779815924167633\n",
      "Train Loss: -0.7802879810333252\n",
      "Train Loss: -0.780140084028244\n",
      "Train Loss: -0.7800689697265625\n",
      "Creating window glfw\n",
      "Episode 21 -> Reward: -50.0\n",
      "Episode 22 -> Reward: -50.0\n",
      "Episode 23 -> Reward: -50.0\n"
     ]
    }
   ],
   "source": [
    "update_target = 0\n",
    "for epi in range(1000):\n",
    "    steps = 0\n",
    "    state = env.reset()['observation']\n",
    "    mu, logvar = vae_model.encode(torch.FloatTensor(state).to(device))\n",
    "    mu, logvar = mu.detach(), logvar.detach()\n",
    "    zg = vae_model.reparameterize(mu, logvar)\n",
    "    done = False\n",
    "    episode = list()\n",
    "    epi_reward = 0\n",
    "    while not done:\n",
    "        to_fwd = torch.cat((mu, zg))\n",
    "        action = select_action(policy, to_fwd)\n",
    "        action = noise.get_action(action, steps)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = next_state['observation']\n",
    "        if epi % 20 == 0:\n",
    "            env.render()\n",
    "        next_mu, _ = vae_model.encode(torch.FloatTensor(next_state).to(device))\n",
    "        next_mu = next_mu.detach()\n",
    "        memory.push(mu.cpu().numpy(), action, reward,\n",
    "                    next_mu.cpu().numpy(), done, zg.detach().cpu().numpy())\n",
    "        episode.append((state, action, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "        data_vae.append(state)\n",
    "        mu = next_mu\n",
    "        update_target += 1\n",
    "        steps += 1\n",
    "        epi_reward += reward\n",
    "    print('Episode', epi, '-> Reward:', epi_reward)\n",
    "    if len(memory) > 128:\n",
    "        for epoch in range(10):\n",
    "            train_policy(policy, crt, tgt_crt, policy_optim,\n",
    "                         crt_optim, memory, vae_model)\n",
    "            soft_sync(crt, tgt_crt)\n",
    "\n",
    "    for i, (state, action, next_state, done) in enumerate(episode):\n",
    "        for t in np.random.choice(len(episode), 5):\n",
    "            s_hi = episode[t][-2]\n",
    "            s_hi, _ = vae_model.encode(\n",
    "                torch.FloatTensor(next_state).to(device))\n",
    "            s_hi = s_hi.detach().cpu().numpy()\n",
    "            mu, _ = vae_model.encode(torch.FloatTensor(state).to(device))\n",
    "            mu = mu.detach().cpu().numpy()\n",
    "            memory.push(mu, action, 0, s_hi, done, s_hi)\n",
    "\n",
    "    if epi % 10 == 0 and epi > 0:\n",
    "        batches = [random.sample(data_vae, 128) for _ in range(10)]\n",
    "        for _ in range(10):\n",
    "            train_vae(batches)\n",
    "            "
   ]
  }
 ]
}