{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from models.bvae import BetaVAE\n",
    "from models.ddpg import OUNoise\n",
    "from models.sac import GaussianPolicy, QNetwork\n",
    "from utils.memory import ReplayBuffer\n",
    "from utils.sync import soft_sync\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FetchReach-v1')"
   ]
  },
  {
   "source": [
    "## Vae training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_training = list()\n",
    "pre_training.append(env.reset()['observation'])\n",
    "for _ in range(100):\n",
    "    status = env.reset()\n",
    "    state = status['observation']\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = next_state['observation']\n",
    "        pre_training.append(next_state)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BetaVAE(\n",
       "  (fc1): Linear(in_features=10, out_features=32, bias=True)\n",
       "  (fc21): Linear(in_features=32, out_features=5, bias=True)\n",
       "  (fc22): Linear(in_features=32, out_features=5, bias=True)\n",
       "  (fc3): Linear(in_features=5, out_features=32, bias=True)\n",
       "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "vae_model = BetaVAE(env.observation_space['observation'].shape[0],\n",
    "                    env.observation_space['observation'].shape[0]//2)\n",
    "vae_model = vae_model.to(torch.device('cuda'))\n",
    "vae_optim = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "vae_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_vae(train_data):\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_data):\n",
    "        data = torch.FloatTensor(data).to(torch.device('cuda'))\n",
    "        vae_optim.zero_grad()\n",
    "        results = vae_model(data)\n",
    "        loss = vae_model.loss_function(*results, M_N=1/len(train_data))\n",
    "        loss['loss'].backward()\n",
    "        train_loss += loss['loss'].item()\n",
    "        vae_optim.step()\n",
    "    print('Train Loss:', train_loss/len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Loss: 0.2206296641857196\n",
      "Train Loss: 0.13651585827271143\n",
      "Train Loss: 0.07836754639179279\n",
      "Train Loss: 0.049603459831231676\n",
      "Train Loss: 0.035828374135188565\n",
      "Train Loss: 0.029814338311553\n",
      "Train Loss: 0.02536992594981805\n",
      "Train Loss: 0.023539976049692202\n",
      "Train Loss: 0.023371421564847995\n",
      "Train Loss: 0.02170574779693897\n",
      "Train Loss: 0.021741728608806927\n",
      "Train Loss: 0.02134656748519494\n",
      "Train Loss: 0.02070120968020115\n",
      "Train Loss: 0.021340196665662985\n",
      "Train Loss: 0.02256954962817522\n",
      "Train Loss: 0.02057242677666438\n",
      "Train Loss: 0.02302084729457513\n",
      "Train Loss: 0.020977917437752087\n",
      "Train Loss: 0.021093307158503778\n",
      "Train Loss: 0.021790261356494367\n",
      "Train Loss: 0.022369756817053527\n",
      "Train Loss: 0.023350448705829106\n",
      "Train Loss: 0.023109545190938007\n",
      "Train Loss: 0.021566733001516417\n",
      "Train Loss: 0.024439557598760493\n",
      "Train Loss: 0.02200858261531744\n",
      "Train Loss: 0.02241434756284341\n",
      "Train Loss: 0.025536162110093314\n",
      "Train Loss: 0.02255405531002161\n",
      "Train Loss: 0.023426195940910242\n",
      "Train Loss: 0.026508762286259577\n",
      "Train Loss: 0.02269691216926544\n",
      "Train Loss: 0.02322960317803499\n",
      "Train Loss: 0.02419461481846296\n",
      "Train Loss: 0.02760129192700753\n",
      "Train Loss: 0.026079188865155745\n",
      "Train Loss: 0.024674238852010325\n",
      "Train Loss: 0.0249226118127505\n",
      "Train Loss: 0.02869400143241271\n",
      "Train Loss: 0.029356928542256355\n",
      "Train Loss: 0.025476824420575913\n",
      "Train Loss: 0.02565003382280851\n",
      "Train Loss: 0.027130266818671655\n",
      "Train Loss: 0.029736179548005264\n",
      "Train Loss: 0.026049768790984765\n",
      "Train Loss: 0.02534205576357169\n",
      "Train Loss: 0.027614850383729506\n",
      "Train Loss: 0.029183621136232827\n",
      "Train Loss: 0.030624710286083896\n",
      "Train Loss: 0.030569303518113416\n"
     ]
    }
   ],
   "source": [
    "batches = np.array_split(np.array(pre_training), len(pre_training)//128)\n",
    "for epoch in range(50):\n",
    "    train_vae(batches)"
   ]
  },
  {
   "source": [
    "## Training policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "lr = 1e-3\n",
    "\n",
    "target_entropy = - \\\n",
    "    torch.prod(torch.Tensor(env.action_space.shape).to(device)).item()\n",
    "log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "alpha_optim = optim.Adam([log_alpha], lr=lr)\n",
    "\n",
    "policy = GaussianPolicy((env.observation_space['observation'].shape[0]//2)*2,\n",
    "                        env.action_space.shape[0]).to(device)\n",
    "\n",
    "crt = QNetwork((env.observation_space['observation'].shape[0]//2)\n",
    "               * 2, env.action_space.shape[0]).to(device)\n",
    "tgt_crt = QNetwork((env.observation_space['observation'].shape[0]//2)*2,\n",
    "                   env.action_space.shape[0]).to(device)\n",
    "\n",
    "tgt_crt.load_state_dict(crt.state_dict())\n",
    "\n",
    "policy_optim = optim.Adam(policy.parameters(), lr=lr)\n",
    "crt_optim = optim.Adam(crt.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = OUNoise(env.action_space)\n",
    "memory = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    x = x.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "    res = np.linalg.norm(x-y, axis=1)\n",
    "    return torch.tensor(res).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(act_net, crt_net, tgt_crt_net,\n",
    "                 optimizer_act, optimizer_crt,\n",
    "                 memory, vae_model, batch_size=128,\n",
    "                 automatic_entropy_tuning=True):\n",
    "    global alpha, log_alpha, alpha_optim\n",
    "    gamma = 0.99\n",
    "    state_batch, action_batch, reward_batch,\\\n",
    "        next_state_batch, mask_batch, goal_batch = memory.sample(batch_size)\n",
    "\n",
    "    state_batch = torch.FloatTensor(state_batch).to(device)\n",
    "    goal_batch = torch.FloatTensor(goal_batch).to(device)\n",
    "    next_state_batch = torch.FloatTensor(\n",
    "        next_state_batch).to(device)\n",
    "    action_batch = torch.FloatTensor(action_batch).to(device)\n",
    "\n",
    "    reward_batch = torch.FloatTensor(\n",
    "        reward_batch).to(device).unsqueeze(1)\n",
    "\n",
    "    mask_batch = torch.BoolTensor(\n",
    "        mask_batch).to(device).unsqueeze(1)\n",
    "\n",
    "    state_batch, logvar = vae_model.encode(state_batch)\n",
    "    state_batch, logvar = state_batch.detach(), logvar.detach()\n",
    "    next_state_batch = vae_model.encode(next_state_batch)[0].detach()\n",
    "\n",
    "\n",
    "    if np.random.rand() > 0.5:\n",
    "        goal_batch = vae_model.reparameterize(state_batch, logvar)\n",
    "\n",
    "    reward_batch = - dist(next_state_batch, goal_batch)\n",
    "    \n",
    "    state_batch = torch.cat((state_batch, goal_batch), 1)\n",
    "    next_state_batch = torch.cat((next_state_batch, goal_batch), 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_action, next_state_log_pi, _ = act_net.sample(\n",
    "            next_state_batch)\n",
    "        qf1_next_target, qf2_next_target = tgt_crt_net(\n",
    "            next_state_batch, next_state_action)\n",
    "        min_qf_next_target = torch.min(\n",
    "            qf1_next_target,\n",
    "            qf2_next_target) - alpha * next_state_log_pi\n",
    "        min_qf_next_target[mask_batch] = 0.0\n",
    "        next_q_value = reward_batch + gamma * (min_qf_next_target)\n",
    "    # Two Q-functions to mitigate\n",
    "    # positive bias in the policy improvement step\n",
    "    qf1, qf2 = crt_net(state_batch, action_batch)\n",
    "    # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
    "    qf1_loss = F.mse_loss(qf1, next_q_value.detach())\n",
    "    # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
    "    qf2_loss = F.mse_loss(qf2, next_q_value.detach())\n",
    "\n",
    "    pi, log_pi, _ = act_net.sample(state_batch)\n",
    "\n",
    "    qf1_pi, qf2_pi = crt_net(state_batch, pi)\n",
    "    min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "    # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
    "    policy_loss = ((alpha * log_pi) - min_qf_pi).mean()\n",
    "\n",
    "    optimizer_crt.zero_grad()\n",
    "    qf1_loss.backward()\n",
    "    optimizer_crt.step()\n",
    "\n",
    "    optimizer_crt.zero_grad()\n",
    "    qf2_loss.backward()\n",
    "    optimizer_crt.step()\n",
    "\n",
    "    optimizer_act.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    optimizer_act.step()\n",
    "\n",
    "    if automatic_entropy_tuning:\n",
    "        alpha_loss = -(log_alpha * (log_pi +\n",
    "                                    target_entropy\n",
    "                                    ).detach()).mean()\n",
    "\n",
    "        alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        alpha_optim.step()\n",
    "\n",
    "        alpha = log_alpha.exp()\n",
    "    else:\n",
    "        alpha_loss = torch.tensor(0.).to(device)\n",
    "\n",
    "    return policy_loss.item(), qf1_loss.item(), \\\n",
    "        qf2_loss.item(), alpha_loss.item()\n",
    "\n",
    "\n",
    "def select_action(policy, state, evaluate=False):\n",
    "    state = state.unsqueeze(0)\n",
    "    if evaluate is False:\n",
    "        action, _, _ = policy.sample(state)\n",
    "    else:\n",
    "        _, _, action = policy.sample(state)\n",
    "    return action.detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vae = collections.deque(maxlen=500000)\n",
    "for data in pre_training:\n",
    "    data_vae.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 0 -> Reward: -50.0\n",
      "Episode 1 -> Reward: -50.0\n",
      "Episode 2 -> Reward: -50.0\n",
      "Episode 3 -> Reward: -50.0\n",
      "Episode 4 -> Reward: -50.0\n",
      "Episode 5 -> Reward: -47.0\n",
      "Episode 6 -> Reward: -50.0\n",
      "Episode 7 -> Reward: -50.0\n"
     ]
    }
   ],
   "source": [
    "update_target = 0\n",
    "for epi in range(1000):\n",
    "    steps = 0\n",
    "    state = env.reset()['observation']\n",
    "    mu, logvar = vae_model.encode(torch.FloatTensor(state).to(device))\n",
    "    mu, logvar = mu.detach(), logvar.detach()\n",
    "    zg = vae_model.reparameterize(mu, logvar).detach()\n",
    "    done = False\n",
    "    episode = list()\n",
    "    epi_reward = 0\n",
    "    while not done:\n",
    "        to_fwd = torch.cat((mu, zg))\n",
    "        action = select_action(policy, to_fwd)\n",
    "        action = noise.get_action(action, steps)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = next_state['observation']\n",
    "        # if epi % 20 == 0:\n",
    "        #     env.render()\n",
    "        next_mu, _ = vae_model.encode(torch.FloatTensor(next_state).to(device))\n",
    "        next_mu = next_mu.detach()\n",
    "        memory.push(state, action, reward,\n",
    "                    next_state, done, zg.cpu().numpy())\n",
    "        episode.append((state, action, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "        data_vae.append(state)\n",
    "        mu = next_mu\n",
    "        update_target += 1\n",
    "        steps += 1\n",
    "        epi_reward += reward\n",
    "    print('Episode', epi, '-> Reward:', epi_reward)\n",
    "    if len(memory) > 128:\n",
    "        for epoch in range(10):\n",
    "            train_policy(policy, crt, tgt_crt, policy_optim,\n",
    "                         crt_optim, memory, vae_model)\n",
    "            soft_sync(crt, tgt_crt)\n",
    "\n",
    "    for i, (state, action, next_state, done) in enumerate(episode):\n",
    "        for t in np.random.choice(len(episode), 5):\n",
    "            s_hi = episode[t][-2]\n",
    "            s_hi, _ = vae_model.encode(\n",
    "                torch.FloatTensor(next_state).to(device))\n",
    "            s_hi = s_hi.detach().cpu().numpy()\n",
    "            memory.push(state, action, 0, next_state, done, s_hi)\n",
    "\n",
    "    if epi % 10 == 0 and epi > 0:\n",
    "        batches = [random.sample(data_vae, 128) for _ in range(10)]\n",
    "        for _ in range(10):\n",
    "            train_vae(batches)\n",
    "            "
   ]
  }
 ]
}