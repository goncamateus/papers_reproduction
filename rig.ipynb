{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('goncareprod': conda)",
   "display_name": "Python 3.7.7 64-bit ('goncareprod': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4272e52c4de0a7f8d5b08f0b04f0eb590fd27aeb04f96d49bd70e439c4400618"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from models.ddpg import Actor, Critic, OUNoise, train\n",
    "from models.vae import VAE, loss_function\n",
    "from utils.memory import ReplayBuffer\n",
    "from utils.sync import soft_sync\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "source": [
    "## Vae training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_training = list()\n",
    "pre_training.append(env.reset())\n",
    "for _ in range(100):\n",
    "    status = env.reset()\n",
    "    state = status\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = next_state\n",
    "        pre_training.append(next_state)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=8, out_features=400, bias=True)\n",
       "  (fc21): Linear(in_features=400, out_features=2, bias=True)\n",
       "  (fc22): Linear(in_features=400, out_features=2, bias=True)\n",
       "  (fc3): Linear(in_features=2, out_features=400, bias=True)\n",
       "  (fc4): Linear(in_features=400, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "vae_model = VAE(env.observation_space.shape[0], env.observation_space.shape[0]//3)\n",
    "vae_model = vae_model.to(torch.device('cuda'))\n",
    "vae_optim = optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "vae_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(train_data):\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_data):\n",
    "        data = torch.FloatTensor(data).to(torch.device('cuda'))\n",
    "        vae_optim.zero_grad()\n",
    "        recon_batch, mu, logvar = vae_model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, env.observation_space.shape[0])\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        vae_optim.step()\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_vae(np.array_split(np.array(pre_training), len(pre_training)/128))"
   ]
  },
  {
   "source": [
    "## Training policy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Actor((env.observation_space.shape[0]//3)*2, env.action_space.shape[0]).to(device)\n",
    "tgt_policy = Actor((env.observation_space.shape[0]//3)*2, env.action_space.shape[0]).to(device)\n",
    "\n",
    "crt = Critic((env.observation_space.shape[0]//3)*2, env.action_space.shape[0]).to(device)\n",
    "tgt_crt = Critic((env.observation_space.shape[0]//3)*2, env.action_space.shape[0]).to(device)\n",
    "\n",
    "tgt_policy.load_state_dict(policy.state_dict())\n",
    "tgt_crt.load_state_dict(crt.state_dict())\n",
    "\n",
    "policy_optim = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "crt_optim = optim.Adam(crt.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = OUNoise(env.action_space)\n",
    "memory = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x, y):\n",
    "    x = x.cpu().numpy()\n",
    "    y = y.cpu().numpy()\n",
    "    res = np.linalg.norm(x-y, axis=1)\n",
    "    return torch.tensor(res).unsqueeze(1).to(device)\n",
    "\n",
    "def train_policy(critic, critic_target, actor, actor_target,\n",
    "          critic_optim, actor_optim, memory, vae_model, batch_size=128):\n",
    "    gamma = 0.99\n",
    "    state_batch, action_batch,\\\n",
    "        reward_batch, next_state_batch, done_batch, goal_batch = memory.sample(batch_size)\n",
    "    state_batch, logvar = vae_model.encode(state_batch)\n",
    "    state_batch, logvar = state_batch.detach(), logvar.detach()\n",
    "\n",
    "    next_state_batch, _ = vae_model.encode(next_state_batch)\n",
    "    next_state_batch = next_state_batch.detach()\n",
    "\n",
    "    # if np.random.rand() > 0.5:\n",
    "    #     goal_batch = vae_model.reparameterize(state_batch, logvar)\n",
    "    \n",
    "    reward_batch = -dist(next_state_batch, goal_batch)\n",
    "\n",
    "    state_batch = torch.cat((state_batch, goal_batch), 1)\n",
    "    next_state_batch = torch.cat((next_state_batch, goal_batch), 1)\n",
    "\n",
    "    actor_loss = critic(state_batch, actor(state_batch))\n",
    "    actor_loss = -actor_loss.mean()\n",
    "\n",
    "    actor_optim.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optim.step()\n",
    "\n",
    "    next_actions_target = actor_target(next_state_batch)\n",
    "    q_targets = critic_target(next_state_batch, next_actions_target)\n",
    "    targets = reward_batch + (1.0 - done_batch)*gamma*q_targets\n",
    "    q_values = critic(state_batch, action_batch.squeeze())\n",
    "    critic_loss = F.smooth_l1_loss(q_values, targets.detach())\n",
    "    critic_optim.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    critic_optim.step()\n",
    "\n",
    "\n",
    "    return actor_loss.item(), critic_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode 0 -> Reward: 19.368908918967108\n",
      "Episode 1 -> Reward: -657.974199951451\n",
      "Episode 2 -> Reward: -426.2489320894064\n",
      "Episode 3 -> Reward: -157.11664422830603\n",
      "Episode 4 -> Reward: -172.7645991584272\n",
      "Episode 5 -> Reward: -293.1431921049948\n",
      "Episode 6 -> Reward: -164.0017742976267\n",
      "Episode 7 -> Reward: -172.51679280823106\n",
      "Episode 8 -> Reward: -125.69591036336519\n",
      "Episode 9 -> Reward: -121.08660769773033\n",
      "Episode 10 -> Reward: -206.54935673958346\n",
      "Episode 11 -> Reward: -134.5543270413786\n",
      "Episode 12 -> Reward: -144.9091936316437\n",
      "Episode 13 -> Reward: -238.3228816102036\n",
      "Episode 14 -> Reward: -99.51586193206555\n",
      "Episode 15 -> Reward: -144.7912014442307\n",
      "Episode 16 -> Reward: -141.39861748278415\n",
      "Episode 17 -> Reward: -108.07974878509836\n",
      "Episode 18 -> Reward: -161.9029844544154\n",
      "Episode 19 -> Reward: -166.42700339187803\n",
      "Episode 20 -> Reward: -116.05401048367271\n",
      "Episode 21 -> Reward: -325.6798483322197\n",
      "Episode 22 -> Reward: -315.0346874389963\n",
      "Episode 23 -> Reward: -697.6764038646947\n",
      "Episode 24 -> Reward: -372.1914685629026\n",
      "Episode 25 -> Reward: -507.71159046285294\n",
      "Episode 26 -> Reward: -520.9392173248054\n",
      "Episode 27 -> Reward: -195.03282809192677\n",
      "Episode 28 -> Reward: -142.1866898405101\n",
      "Episode 29 -> Reward: -456.12872606646727\n",
      "Episode 30 -> Reward: -329.77977457673387\n",
      "Episode 31 -> Reward: -272.4167002479367\n",
      "Episode 32 -> Reward: -226.7501559960946\n",
      "Episode 33 -> Reward: -171.91501068256616\n",
      "Episode 34 -> Reward: -425.730960867539\n",
      "Episode 35 -> Reward: -97.34469749203987\n",
      "Episode 36 -> Reward: -154.3920252254324\n",
      "Episode 37 -> Reward: -179.3215720246132\n",
      "Episode 38 -> Reward: -286.9093185581876\n",
      "Episode 39 -> Reward: -288.4065694056935\n",
      "Episode 40 -> Reward: -388.0266657308732\n",
      "Episode 41 -> Reward: -369.129015095912\n",
      "Episode 42 -> Reward: -67.6552297944795\n",
      "Episode 43 -> Reward: -395.8453962645585\n",
      "Episode 44 -> Reward: -272.55684520571106\n",
      "Episode 45 -> Reward: -486.651908333956\n",
      "Episode 46 -> Reward: -506.52208960824106\n",
      "Episode 47 -> Reward: -113.73719117815102\n",
      "Episode 48 -> Reward: -31.46053889357553\n"
     ]
    }
   ],
   "source": [
    "data_vae = collections.deque(maxlen=500000)\n",
    "for data in pre_training:\n",
    "    data_vae.append(data)\n",
    "\n",
    "for epi in range(1000):\n",
    "    update_target = 0\n",
    "    steps = 0\n",
    "    state = env.reset()\n",
    "    mu, logvar = vae_model.encode(torch.FloatTensor(state).to(device))\n",
    "    mu, logvar = mu.detach(), logvar.detach()\n",
    "    zg = vae_model.reparameterize(mu, logvar)\n",
    "    done = False\n",
    "    episode = list()\n",
    "    epi_reward = 0\n",
    "    while not done:\n",
    "        to_fwd = torch.cat((mu, zg))\n",
    "        action = policy.get_action(to_fwd)\n",
    "        action = noise.get_action(action, steps)[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if epi%20 == 0 and epi > 0:\n",
    "            env.render()\n",
    "        next_state = next_state\n",
    "        done = 1 if done else 0\n",
    "        memory.put((state, action, reward, next_state, done, zg.detach().cpu().numpy()))\n",
    "        episode.append((state, action, next_state, done))\n",
    "        \n",
    "        if memory.size() > 128:\n",
    "            act_loss, crt_loss = train_policy(crt, tgt_crt, policy, tgt_policy, crt_optim, policy_optim, memory, vae_model)\n",
    "            if update_target%2==0 and update_target > 0:\n",
    "                soft_sync(policy, tgt_policy)\n",
    "                soft_sync(crt, tgt_crt)\n",
    "        \n",
    "        state = next_state\n",
    "        data_vae.append(state)\n",
    "        mu, _ = vae_model.encode(torch.FloatTensor(next_state).to(device))\n",
    "        mu = mu.detach()\n",
    "        update_target += 1\n",
    "        steps += 1\n",
    "        epi_reward += reward\n",
    "    print('Episode', epi, '-> Reward:', epi_reward)\n",
    "\n",
    "    for i, (state, action, next_state, done) in enumerate(episode):\n",
    "        for t in np.random.choice(len(episode), 20):\n",
    "            s_hi = episode[t][-2]\n",
    "            s_hi = vae_model.encode(torch.FloatTensor(next_state).to(device))[0].detach().cpu().numpy()\n",
    "            memory.put((state, action, 0, next_state, done, s_hi))\n",
    "   \n",
    "    \n",
    "    if epi%50 == 0 and epi > 0:\n",
    "        batches = [random.sample(data_vae, 128) for _ in range(10)]\n",
    "        for _ in range(10):\n",
    "            train_vae(batches)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}